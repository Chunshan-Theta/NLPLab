 在2012年10月發行的《哈佛商業評論》中，戴文波特‧湯姆斯（Thomas Patil）發表了一篇文章，描述「21世紀最性感的職業—資料科學家（Data Century）」。同年美國歐巴馬政府更投資了近兩億美元推行「大數據的研究與發展計畫（The Initiative）」，希望藉著提升從大型複雜的資料中提取知識的能力，能加快科學和工程的開發並保障國家安全。

2015年2月19日，白宮正式任命帕蒂爾為首位首席資料科學家。當天他在聖荷西（San 2015會議做了一場主題演講，講題是「資料科學：我們將邁向何方？（Data Going）」，美國總統歐巴馬還特地錄製短片祝賀大會順利舉行。影片中歐巴馬呼籲：「我們需要你—資料科學家—來幫助國民建立更好的數位服務，幫助我們揭開更新的創意……幫助我們改善這個國家和全世界。」

資料科學與大數據

在這個大數據時代，資料科學的狂潮不斷地推動著這個世界。2015年4月，美國國家標準技術研究所（National NIST）發表了共包括7冊資料的「大數據互用性架構草案」。在第一冊定義篇中，資料科學被譽為新興的第四個科學典範（理論科學、實驗科學、計算科學與資料科學）：「資料科學是一透過完整資料生命周期流程，所產生的自原始資料到具行動力的知識的實驗性綜合體。」

資料科學，這第四個科學典範是2007年由格雷‧吉姆（Jim Gray）所命名，它代表直接由資料本身所產生的知識。NIST所定義的資料科學典範是「直接由資料，並透過一系列發現、假設與假設檢定的流程，萃取出具行動力的知識」。

所謂「Big Data」坊間有許多翻譯，包括大數據、巨量資料、海量資料等。NIST則定義為：由具有龐大資料量、高速度、多樣性（多重異質資料格式）、變異性等特徵的資料集所組成，它需要可擴延的架構來進行有效儲存、處理與分析。

巨量資料的特徵

今日可說是個大數據的時代！自從進入21世紀後，全球資料量呈現大爆炸式的增長，資料量從PB級躍升至ZB級。根據國際資料公司（IDC）發布的2012年研究報告，從2011年全球創建和複製的資料總量是1.8 ZB，並以每兩年增加一倍的速度快速增長。預計到2020年，全球產生的資料總量將超過 ZB，這是地球上所有海灘上沙粒數量的57倍。

谷歌（Google）公司每天處理超過24 PB。淘寶網有5億多名會員，線上商品超過10億件，每天交易平均金額高達新台幣6億元以上，每日所產生的資料量也超過 TB以上，然而這只是全球資料量的一小部分。

大數據時代產生的資料有許多特徵，這些特徵也引領資料科學在這些新興資料型態的分析上有著重大發展。巨量資料的最大特徵當然就是龐大的資料量，如一般桌上型電腦的記憶體是以GB為計量單位，硬碟的容量則是以TB為主。電腦運算須把資料載入到記憶體上，因此要處理龐大的PB或EB資料，就必須有新的儲存模式及計算模式，這也是資料科學的重要研發領域。

巨量資料的第二大特徵就是速度。高速有兩層涵義，第一層是資料產生的速度，每天社交網路Facebook、Twitter及通訊軟體Line所產生的資料就是一例。IDC指出，到2020年，全球所有資訊部門擁有伺服器的總量將較目前多出10倍，管理的資料也比現在多出50倍，全球將總共有35ZB的資料量。另一層則是處理的速度要求，以中國大陸淘寶網在每年11月11日光棍節的電子商務活動為例，淘寶網須針對交易資料即時呈現活動的交易現況，這是巨量資料分析的一大挑戰。

巨量資料通常有時效性，一旦傳送到運算伺服器，就要能即時取得分析結果才能發揮其最大價值。巨量資料的即時分析需要飛秒級的速度，甚至1秒內完成億萬級資料的處理和分析，這也是巨量資料分析的挑戰課題。

資料多樣性是巨量資料的第三大特徵。一般商業交易所使用的資料大抵是以結構化資料為主，透過預先定義好的資料欄位進行儲存與運算。但除了結構化資料外，巨量資料還包含許多半結構化或非結構化資料。這些資料包括各類型生產機台所產生的日誌檔案、各式網路設備與伺服器產生的網路日誌檔、聲音、影片、圖片、地理位置資訊等，這類型資料的儲存與運算都需要新的運算架構。

上述所指的巨量資料主要以龐大的資料量（volume）、速度（velocity）與資料多樣性（variety）三大特徵為主，這就是大數據所謂的3V特徵。後續也有許多學研單位指出其他特徵包括真實性、價值與視覺化，這也凸顯了巨量資料的多面向觀點。

挑戰性課題與解決方案

龐大、高度異質性與非結構化資料的第一個挑戰就是資料儲存架構。現今資料庫技術主要是1970年卡德（E.F. Codd）所提出的關聯式資料庫，不管是開放原始碼的Postgresql、MySQL或商業版的Oracle、IBM DB2與微軟MSSQL資料庫，都是以關聯式資料庫的理論架構為主。這些資料庫在面對新興的高度異質性與非結構化資料都面臨了極大的挑戰，因此有許多新興的資料庫技術被提出，這些不同的資料庫技術都通稱為NoSQL（Not SQL）。

NoSQL與傳統式資料庫有相當多的不同，後者以定義良好的資料庫綱要來組織與儲存資料，並利用高階的SQL查詢語法存取資料。但前者無固定綱要模式，且NoSQL也不使用一般認知的SQL查詢語法。這些不同資料庫設計都有其目的與使用場景，最主要的設計考量包括可擴展性資料儲存功能的需求。

NoSQL資料庫是現今資料庫技術研發的重要領域之一。在超過上百個NoSQL資料庫中，大致可區分為五大類型，分別是鍵—值儲存資料庫、圖形資料庫、文件儲存資料庫、記憶體資料庫與欄儲存資料庫。這些針對資料多樣性與可擴展性所設計的資料庫技術，是資訊領域必須掌握的新技術。

針對龐大資料量進行運算，最常見與直覺的做法便是利用多台電腦（電腦叢集）來運算，這是分散式運算著重的主題。要進行電腦叢集的運算，通常需要有分散式檔案系統來儲存計算過程中的資料，以方便不同電腦快速且一致地存取資料。另外，不同台電腦運算所得的結果可能出錯，部分電腦也有當機的風險，因此要讓計算結果無誤，便需要新的容錯架構來處理。巨量資料的資料匯入、資料預先處理、資料分析與資料探勘都是巨量資料分析平台必須深入考量的課題。

面對龐大資料量的分析，必須特別注重穩定性與系統平台的可擴延性。現今巨量資料分析有許多架構，從最早廣受喜愛的Hadoop生態體系到新進的Apache Spark、Storm都有不同的擁護者。Hadoop是Apache軟體基金會眾多開放原始碼的專案之一，也是現今使用度最高的巨量資料分析平台，它主要特色在於分散式運算和分散式檔案系統，整合了許多軟體及元件共同組成了一個Hadoop生態體系，具有高可用性、高擴充性、高效率、高容錯性等優點，因此廣受喜愛。

2003年當谷歌發表GFS分散式檔案系統的核心技術論文後，次年卡丁‧道格（Doug Cutting）根據這理論撰寫出HDFS分散式檔案系統，同年谷歌再發表Map Reduce核心運算模式，而卡丁‧道格與同事也立即實作出運算架構。

由於卡丁等人曾聽見小孩為小象玩具取名為Hadoop，索性就把Hadoop做為專案名稱。Hadoop在2008年成為Apache頂級專案，這專案主要核心技術可分為3個子項目，即Hadoop System（HDFS）分散式檔案系統和MapReduce分散式運算架構及資源調度子模組Yet Negotiator（YARN）。

HDFS分散式檔案系統把分散的儲存資源整合成一個具有高容錯性、高擴充性、高吞吐率等優點，且允許使用者把Hadoop建置在低廉的硬體上的檔案系統。HDFS是主從架構，它由兩種角色組成：命名節點及資料節點。命名節點負責檔案系統中各個檔案屬性權限等資訊的管理及儲存，資料節點則擔任運算的任務。

一個龐大的資料檔案會被切割成數個較小的區塊，儲存在不同的資料節點上，每一個區塊還會有數份副本存放在不同節點，因此當其中一個節點損壞時，檔案系統中的資料還能保存無缺。命名節點還需要記錄每一份檔案存放的位置，當有存取檔案的需求時，要協調資料節點來負責回應。當有節點損壞時，命名節點也會自動進行資料的搬遷和複製。

Hadoop的分散式運算是建立在MapReduce計算模式，其主架構是把龐大資料切割成許多部分資料，然後交給多台電腦先進行Map運算後，再做Reduce運算來進行巨量資料的平行化快速處理。在Hadoop中是以Java編寫程式，其以Java開發的MapReduce模式稱為native mode（原生模式），若要以不同程式如Python或R進行分析，也可利用Hadoop Streaming模式開發。

Hadoop隨著發展推出第2代的Yarn資源調度與管理的新框架。Yarn把負責運算工作分配與追蹤的任務分開來，它使用資源管理者負責管理分配全局資源，並使用應用程式主導者負責管理任務的整個生命周期內的所有事宜，任務執行的追蹤與管理則使用節點管理者。整體運作架構是由應用程式主導者與資源管理者溝通協商如何分配資源，和節點管理者協同執行並監測應用程式的執行情況。

Hadoop生態體系除了上述三大核心外，也包括了下列子平台。其中HBase是用於Hadoop檔案系統上的資料庫系統，採取Column-Oriented 資料庫設計。Hive則是建置在HDFS上的一套分散式資料倉儲系統，可用SQL語法存取Hadoop資料，另可讓使用者以慣用的SQL語法存取Hadoop檔案中的大型資料集。

ZooKeeper則是監控和協調Hadoop分散式運作的集中式服務，可提供各個伺服器的配置和運作狀態資訊，用於提供不同Hadoop系統角色之間的工作協調。Pig則利用更接近使用者的介面，封裝了HDFS和MapReduce中低層的程式設計介面，使用者使用一些敘述就能完成工作，而不需撰寫較為複雜的Map Reduce框架程式，這就降低了使用者開發的難度，也為系統自動進行最佳化。

Sqoop主要是協助傳統關聯式資料庫與 Hadoop之間進行高效率的大規模資料交換，透過 Sqoop可以輕鬆地在指令模式下把資料導入到Hadoop與其相關系統（如HBase、Hive）。Mahout則提供具高度可擴充性的機器學習演算法，它包括群集、分類，以及協同過濾的核心演算法。

近年興起的Storm是Twitter公司在2011年7月收購的BackType社交媒體資料分析公司，其首席工程師馬茲‧內森（Nathan Marz）與其團隊於同年9月17日推出的第一個作品Storm，是一個分散式串流資料處理系統，其強大的分散式管理、可靠性高、高容錯保障，在性能和功能方面大幅彌補Hadoop所欠缺的即時運算需求，使得很多公司紛紛加入使用。

Clojure是Storm主要的開發語言，它是基於Lisp所設計的函數型語言，並且增加了多執行緒等特性，使得Storm在底層有著高效率的通信和非同步處理能力。另一個著名的巨量資料分析平台是Spark，2009年由扎哈里亞‧馬泰（Matei Zaharia）在加州大學柏克萊分校AMP Lab所開發，2010年以Scala語言開發完成，並於同年透過BSD授權條款開源釋出。2013年該專案捐贈給Apache軟體基金會，並切換授權條款至Apache2.0。

2014年2月Cloudera宣稱加入Spark，2014年4月MapR也投入Spark陣營。另Apache Mahout也放棄MapReduce模式，將改用Spark為計算引擎。2014年11月Databricks團隊使用Spark刷新資料排序的世界紀錄而引起全球注目。

Spark使用了記憶體內運算技術，能在資料尚未寫入硬碟時，就在記憶體內分析運算，甚至透過Spark的串流處理套件即時處理串流資料。根據Apache Spark官方的說明，Spark在記憶體內執行程式的運算速度，可以比Hadoop MapReduce的運算速度還快上100倍，即便是在硬碟執行時，Spark也有達到10倍的速度。

面對巨量資料時代的作為

我國及全球已經有許多巨量資料分析的應用領域與成功案例。以半導體產業為例，各種先進製程控制的資料分析至為關鍵，一旦製程良率出問題，如何在最短的時間內找出所有相關因素，甚至事先就能預知並且杜絕問題發生，一直是高科技製造業最大的挑戰。面對各種機台產生的大數據，巨量資料分析就扮演著重要角色。

另外在全球興起的物聯網、車聯網及智慧城市，再加上各國政府力推的開放政府與開放資料，都會產生許多龐大的資料，這些資料如何產生有價值的訊息、如何有效管理與利用，更是現今資料治理的重大議題。

各行各業對於大數據的濃厚興趣，也直接反映在大數據人才的豐厚薪資中。McKinsey 萬名的大數據分析師。在美國芝加哥獵人頭公司Linda 2014年的薪資報告中，也發現資料科學家的平均年薪資已優於醫師和律師，而美國人力資源網站 SimplyHires.com與Linkedin上則約有 24,000～36,000個資料科學職務求才名額。這些報導都不斷強調嚴重欠缺的資料科學家或大數據資料分析師將是各國爭相搶奪的人才。

資料科學家或大數據資料分析師都是近三年才冒出頭的職務，學術界尚未培育出足夠業界所需的人力，因此現在大多是以其他領域的人來從事資料分析工作。譬如雅虎的資料分析工作是由天體物理學、應用數學背景的人擔任；行動付款公司Square則僱用認知心理學家來研究消費者的付款行為模式。

大數據時代的到來及巨量資料分析的龐大需求，不管是大資料城市治理，還是智慧聯網的各種應用，都宣示著需要更多的人才投入。這也是現今台灣朝向加值創新智慧島之路邁進時，須及早準備的議題。


